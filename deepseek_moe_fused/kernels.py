"""
Fused DeepSeek-MoE Triton Kernel Implementation.

This module contains the core Triton kernel that performs the "DeepSeek Fusion":
- Load input X ONCE into SRAM via Virtual Gathering
- Compute both Routed Expert GEMM and Shared Expert GEMM
- Accumulate in FP32 registers, write back in BF16

The key optimization is that we eliminate the second HBM read for the shared
expert by keeping X in L1 cache while computing both paths.

Memory Bandwidth Analysis:
-------------------------
Naive approach:
  - Load X for routed: N * H * 2 bytes (BF16)
  - Load X for shared: N * H * 2 bytes (BF16)  
  - Total: 2 * N * H * 2 bytes

Fused approach:
  - Load X once: N * H * 2 bytes (BF16)
  - Total: N * H * 2 bytes

Theoretical speedup: ~2x on memory-bound workloads (large batch sizes)
"""

import triton
import triton.language as tl
import torch


@triton.jit
def fused_moe_forward_kernel(
    # ==========================================================================
    # Pointer Arguments (Base addresses of tensors in HBM)
    # ==========================================================================
    X_ptr,              # Input activations: [N, H] in BF16
    W_routed_ptr,       # Routed expert weights: [E, H, D] in BF16
    W_shared_ptr,       # Shared expert weights: [H, D] in BF16
    Y_ptr,              # Output buffer: [total_assignments, D] in BF16 (sorted order)
    
    # Routing metadata (generated by compute_routing_metadata)
    sorted_token_indices_ptr,   # [total_assignments] int32 - Row indices into X
    sorted_expert_indices_ptr,  # [total_assignments] int32 - Expert ID for each assignment
    
    # ==========================================================================
    # Dimension Arguments
    # ==========================================================================
    total_assignments,  # N * top_k
    H,                  # Hidden dimension (input)
    D,                  # Intermediate dimension (output)
    
    # ==========================================================================
    # Stride Arguments (for pointer arithmetic)
    # ==========================================================================
    stride_xn,          # Stride to move one row in X (= H for contiguous)
    stride_xh,          # Stride to move one column in X (= 1 for contiguous)
    stride_we,          # Stride between experts (= H * D)
    stride_wh,          # Stride along hidden dim (= D)
    stride_wd,          # Stride along output dim (= 1)
    stride_sh,          # Stride along hidden dim for shared (= D)
    stride_sd,          # Stride along output dim for shared (= 1)
    stride_yn,          # Stride to move one row in Y
    stride_yd,          # Stride along output dim in Y
    
    # ==========================================================================
    # Block Size Meta-Parameters (compile-time constants)
    # ==========================================================================
    BLOCK_M: tl.constexpr,  # Tokens per block (e.g., 32)
    BLOCK_N: tl.constexpr,  # Output dimension tile (e.g., 64)
    BLOCK_K: tl.constexpr,  # Hidden dimension tile for K-loop (e.g., 32)
):
    """
    Fused MoE Forward Kernel with Virtual Gathering.
    
    Grid Structure (2D):
    - pid_m: Which token block (0 to ceil(total_assignments / BLOCK_M) - 1)
    - pid_n: Which output tile (0 to ceil(D / BLOCK_N) - 1)
    
    Each program computes a [BLOCK_M, BLOCK_N] tile of the output.
    """
    
    # ==========================================================================
    # STEP 1: Get Program IDs (2D Grid)
    # ==========================================================================
    pid_m = tl.program_id(0)  # Token block index
    pid_n = tl.program_id(1)  # Output tile index
    
    # ==========================================================================
    # STEP 2: Compute Token and Output Ranges
    # ==========================================================================
    # Token range for this block
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    # Output column range for this block
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    
    # Masks for boundary conditions
    mask_m = offs_m < total_assignments
    mask_n = offs_n < D
    
    # ==========================================================================
    # STEP 3: Load Routing Information (Virtual Gather Setup)
    # ==========================================================================
    # Load the original token indices for virtual gathering
    # sorted_token_indices tells us: for assignment i, read row X[sorted_token_indices[i]]
    row_indices = tl.load(
        sorted_token_indices_ptr + offs_m,
        mask=mask_m,
        other=0,
    )
    
    # Load the expert IDs for each assignment
    expert_ids = tl.load(
        sorted_expert_indices_ptr + offs_m,
        mask=mask_m,
        other=0,
    )
    
    # ==========================================================================
    # STEP 4: Initialize Accumulator
    # ==========================================================================
    # FP32 accumulator for numerical stability
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # ==========================================================================
    # STEP 5: K-Loop - Iterate Over Hidden Dimension
    # ==========================================================================
    # For each K-tile, we:
    # 1. Load X tile via virtual gather (ONCE)
    # 2. Load W_routed tiles (per-expert)
    # 3. Load W_shared tile (broadcast)
    # 4. Accumulate: acc += X @ W_routed + X @ W_shared
    
    for k_start in range(0, H, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        mask_k = offs_k < H
        
        # ==================================================================
        # STEP 5a: Virtual Gather - Load X Tile
        # ==================================================================
        # X[row_indices[m], k] for m in [0, BLOCK_M), k in [k_start, k_start+BLOCK_K)
        #
        # Pointer calculation:
        #   X_ptr + row_indices[m] * stride_xn + offs_k * stride_xh
        #
        # row_indices[:, None] broadcasts to [BLOCK_M, BLOCK_K]
        # offs_k[None, :] broadcasts to [BLOCK_M, BLOCK_K]
        X_ptrs = (
            X_ptr
            + row_indices[:, None] * stride_xn   # Scattered row access
            + offs_k[None, :] * stride_xh        # Contiguous column access
        )
        
        X_tile = tl.load(
            X_ptrs,
            mask=mask_m[:, None] & mask_k[None, :],
            other=0.0,
        ).to(tl.float32)
        
        # ==================================================================
        # STEP 5b: Load Shared Weights (Same for All Tokens)
        # ==================================================================
        # W_shared[k, n] for k in offs_k, n in offs_n
        W_shared_ptrs = (
            W_shared_ptr
            + offs_k[:, None] * stride_sh
            + offs_n[None, :] * stride_sd
        )
        
        W_shared_tile = tl.load(
            W_shared_ptrs,
            mask=mask_k[:, None] & mask_n[None, :],
            other=0.0,
        ).to(tl.float32)
        
        # ==================================================================
        # STEP 5c: Load Routed Weights (Per-Expert)
        # ==================================================================
        # This is trickier because different tokens may go to different experts.
        # We need to load the correct expert weights for each token.
        #
        # W_routed[expert_ids[m], k, n]
        #
        # Compute base pointers for each token's expert:
        # W_routed_ptr + expert_ids[m] * stride_we + offs_k * stride_wh + offs_n * stride_wd
        
        W_routed_ptrs = (
            W_routed_ptr
            + expert_ids[:, None, None] * stride_we  # [BLOCK_M, 1, 1]
            + offs_k[None, :, None] * stride_wh      # [1, BLOCK_K, 1]
            + offs_n[None, None, :] * stride_wd      # [1, 1, BLOCK_N]
        )  # Shape: [BLOCK_M, BLOCK_K, BLOCK_N]
        
        # Load routed weights - shape [BLOCK_M, BLOCK_K, BLOCK_N]
        W_routed_tiles = tl.load(
            W_routed_ptrs,
            mask=mask_m[:, None, None] & mask_k[None, :, None] & mask_n[None, None, :],
            other=0.0,
        ).to(tl.float32)
        
        # ==================================================================
        # STEP 5d: Compute Fused GEMM
        # ==================================================================
        # For standard GEMM: acc += X_tile @ W_tile
        # But here each token has its own W_routed slice.
        #
        # For token m:
        #   routed_out[m, n] = sum_k X_tile[m, k] * W_routed_tiles[m, k, n]
        #   shared_out[m, n] = sum_k X_tile[m, k] * W_shared_tile[k, n]
        #
        # Routed: element-wise multiply X_tile[m,k] * W_routed_tiles[m,k,n], sum over k
        # X_tile: [BLOCK_M, BLOCK_K] -> [BLOCK_M, BLOCK_K, 1]
        # W_routed_tiles: [BLOCK_M, BLOCK_K, BLOCK_N]
        routed_contrib = tl.sum(X_tile[:, :, None] * W_routed_tiles, axis=1)  # [BLOCK_M, BLOCK_N]
        
        # Shared: standard matmul X_tile @ W_shared_tile
        shared_contrib = tl.dot(X_tile, W_shared_tile)  # [BLOCK_M, BLOCK_N]
        
        # Accumulate both contributions (THE FUSION!)
        acc += routed_contrib + shared_contrib
    
    # ==========================================================================
    # STEP 6: Store Results
    # ==========================================================================
    # Write output in sorted order
    # Y[offs_m, offs_n] = acc
    Y_ptrs = (
        Y_ptr
        + offs_m[:, None] * stride_yn
        + offs_n[None, :] * stride_yd
    )
    
    # Cast to BF16 and store
    tl.store(
        Y_ptrs,
        acc.to(tl.bfloat16),
        mask=mask_m[:, None] & mask_n[None, :],
    )


def fused_moe_forward(
    X: torch.Tensor,                    # [N, H] BF16
    W_routed: torch.Tensor,             # [E, H, D] BF16
    W_shared: torch.Tensor,             # [H, D] BF16
    sorted_token_indices: torch.Tensor, # [total_assignments] int32
    sorted_expert_indices: torch.Tensor,# [total_assignments] int32
    expert_token_offsets: torch.Tensor, # [E+1] int32 (unused in new kernel but kept for API)
    total_blocks: int,                  # unused in new kernel
    num_experts: int,                   # unused in new kernel
    BLOCK_M: int = 32,
    BLOCK_N: int = 64,
    BLOCK_K: int = 32,
) -> torch.Tensor:
    """
    Python wrapper for the fused MoE forward kernel.
    
    Args:
        X: Input activations [N, H] in BF16
        W_routed: Routed expert weights [E, H, D] in BF16
        W_shared: Shared expert weights [H, D] in BF16
        sorted_token_indices: Metadata map for virtual gathering [total_assignments]
        sorted_expert_indices: Expert ID for each assignment [total_assignments]
        expert_token_offsets: (kept for API compatibility)
        total_blocks: (kept for API compatibility)
        num_experts: (kept for API compatibility)
        BLOCK_M, BLOCK_N, BLOCK_K: Tile sizes
    
    Returns:
        Y: Output activations [total_assignments, D] in BF16 (sorted order)
    """
    N, H = X.shape
    E, _, D = W_routed.shape
    total_assignments = sorted_token_indices.shape[0]
    
    # Allocate output buffer in sorted order
    Y = torch.empty(
        (total_assignments, D),
        dtype=torch.bfloat16,
        device=X.device,
    )
    
    # 2D grid: (token_blocks, output_tiles)
    grid = (
        triton.cdiv(total_assignments, BLOCK_M),
        triton.cdiv(D, BLOCK_N),
    )
    
    fused_moe_forward_kernel[grid](
        # Pointers
        X, W_routed, W_shared, Y,
        sorted_token_indices, sorted_expert_indices,
        # Dimensions
        total_assignments, H, D,
        # Strides for X [N, H]
        X.stride(0), X.stride(1),
        # Strides for W_routed [E, H, D]
        W_routed.stride(0), W_routed.stride(1), W_routed.stride(2),
        # Strides for W_shared [H, D]
        W_shared.stride(0), W_shared.stride(1),
        # Strides for Y [total_assignments, D]
        Y.stride(0), Y.stride(1),
        # Block sizes
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
    )
    
    return Y